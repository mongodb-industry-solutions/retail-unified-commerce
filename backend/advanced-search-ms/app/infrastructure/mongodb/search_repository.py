# app/infrastructure/mongodb/search_repository.py
"""
MongoSearchRepository
=====================

Clean‚ÄëArchitecture adapter that turns **application‚Äëlayer requests**
into MongoDB aggregation pipelines, executes them, and returns
domain‚Äëfriendly structures.

Key responsibilities
--------------------
1. üóÇ  **Pipeline selection** ‚Äì¬†chooses the right builder (keyword, text,
   vector, hybrid‚ÄëRRF) based on the use‚Äëcase.
2. ‚öôÔ∏è  **Configuration injection** ‚Äì¬†passes index names, vector field,
   paging, weights, etc., to each builder.
3. ‚è±  **Pagination logic** ‚Äì computes `skip` & `limit` once, reuses it.
4. üßπ  **Post‚Äëprocessing** ‚Äì filters `inventorySummary` so each store only
   sees its own stock (privacy + payload reduction).

Why keep pipelines in¬†`pipelines/*`?
------------------------------------
* **Single responsibility** ‚Äì this repo focuses on orchestration;
  builders focus on aggregation syntax.
* **Unit testing** ‚Äì pipelines become pure functions that can be tested
  without hitting MongoDB.
* **Re‚Äëusability** ‚Äì the same builder can be embedded in a hybrid RRF
  pipeline, cached, or benchmarked in isolation.
"""

from __future__ import annotations

import logging
from typing import Dict, List, Optional, Tuple

from motor.motor_asyncio import AsyncIOMotorCollection

from app.application.ports import SearchRepository
from app.infrastructure.mongodb.client import MongoClient
from app.infrastructure.mongodb.utils import (
    PRODUCT_FIELDS,           # üîñ single source‚Äëof‚Äëtruth projection
    filter_inventory_summary  # üßπ strip other stores' stock info
)
from app.infrastructure.mongodb.pipelines import (
    build_keyword_pipeline,
    build_text_pipeline,
    build_vector_pipeline,
    build_hybrid_rrf_pipeline,
)
from app.shared.exceptions import InfrastructureError

logger = logging.getLogger("advanced-search-ms.mongo-repo")


# --------------------------------------------------------------------------- #
# üèó  Repository implementation                                               #
# --------------------------------------------------------------------------- #
class MongoSearchRepository(SearchRepository):
    """
    Concrete adapter behind the SearchRepository port (Clean Architecture).

    It delegates *heavy* work to dedicated pipeline builders; this class only
    orchestrates parameters, runs the aggregation, and applies final shaping.
    """

    # --------------------------------------------------------------------- #
    # Construction                                                          #
    # --------------------------------------------------------------------- #
    def __init__(
        self,
        collection: AsyncIOMotorCollection,
        index_name_text: str,
        index_name_vector: str,
        embedding_field: str,
        mongo_client_helper: MongoClient,
    ) -> None:
        self.col           = collection
        self.text_index    = index_name_text
        self.vector_index  = index_name_vector
        self.vector_field  = embedding_field
        self._vec_helper   = mongo_client_helper

        logger.info(
            "[SearchRepo] ‚úÖ Initialised | text_index=%s | vector_index=%s",
            self.text_index, self.vector_index
        )

    # --------------------------------------------------------------------- #
    # Option¬†1 ‚Äì¬†Keyword / regex search                                     #
    # --------------------------------------------------------------------- #
    async def search_keyword(
        self,
        query: str,
        store_object_id: str,
        page: int,
        page_size: int,
    ) -> Tuple[List[Dict], int]:
        logger.info("[SearchRepo] üîé Keyword search | q='%s' | store=%s", query, store_object_id)

        skip = (page - 1) * page_size
        pipeline = build_keyword_pipeline(
            query=query,
            store_object_id=store_object_id,
            skip=skip,
            limit=page_size,
            projection_fields=PRODUCT_FIELDS,
        )
        return await self._run_pipeline(pipeline, store_object_id)

    # --------------------------------------------------------------------- #
    # Option¬†2 ‚Äì¬†Atlas Search text index                                    #
    # --------------------------------------------------------------------- #
    async def search_atlas_text(
        self,
        query: str,
        store_object_id: str,
        page: int,
        page_size: int,
    ) -> Tuple[List[Dict], int]:
        logger.info("[SearchRepo] üîé Text search | q='%s' | store=%s", query, store_object_id)

        skip = (page - 1) * page_size
        pipeline = build_text_pipeline(
            query=query,
            store_object_id=store_object_id,
            text_index=self.text_index,
            skip=skip,
            limit=page_size,
            projection_fields=PRODUCT_FIELDS,
        )
        return await self._run_pipeline(pipeline, store_object_id)

    # --------------------------------------------------------------------- #
    # Option¬†3 ‚Äì¬†Lucene vector search                                       #
    # --------------------------------------------------------------------- #
    async def search_by_vector(
        self,
        embedding: List[float],
        store_object_id: str,
        page: int,
        page_size: int,
    ) -> Tuple[List[Dict], int]:
        logger.info("[SearchRepo] üîé Vector search | store=%s", store_object_id)

        skip = (page - 1) * page_size
        pipeline = build_vector_pipeline(
            embedding=embedding,
            store_object_id=store_object_id,
            vector_index=self.vector_index,
            vector_field=self.vector_field,
            skip=skip,
            limit=page_size,
            projection_fields=PRODUCT_FIELDS,
        )
        return await self._run_pipeline(pipeline, store_object_id)

    # --------------------------------------------------------------------- #
    # Option¬†4 ‚Äì¬†Hybrid Reciprocal Rank Fusion                              #
    # --------------------------------------------------------------------- #
    async def search_hybrid_rrf(
        self,
        query: str,
        embedding: List[float],
        store_object_id: str,
        page: int,
        page_size: int,
        weight_vector: Optional[float] = None,
        weight_text: Optional[float]   = None,
    ) -> Tuple[List[Dict], int]:
        logger.info("[SearchRepo] üîé Hybrid RRF | q='%s' | store=%s", query, store_object_id)

        skip    = (page - 1) * page_size
        weights = {
            "vectorPipeline": weight_vector or 5.0,
            "textPipeline":   weight_text   or 5.0,
        }

        pipeline = build_hybrid_rrf_pipeline(
            query=query,
            embedding=embedding,
            store_object_id=store_object_id,
            text_index=self.text_index,
            vector_index=self.vector_index,
            vector_field=self.vector_field,
            weights=weights,
            skip=skip,
            limit=page_size,
            projection_fields=PRODUCT_FIELDS,
        )
        return await self._run_pipeline(pipeline, store_object_id)

    # --------------------------------------------------------------------- #
    # üîÑ Shared helper ‚Äì run aggregation & post‚Äëprocess                     #
    # --------------------------------------------------------------------- #
    async def _run_pipeline(
        self,
        pipeline: List[Dict],
        store_object_id: str,
    ) -> Tuple[List[Dict], int]:
        """
        Executes the given aggregation pipeline and:

        1. Converts the cursor to a single root doc
        2. Removes other stores' inventory rows (`filter_inventory_summary`)
        3. Returns `(docs, total)` for the application layer
        """
        try:
            logger.debug("[SearchRepo] ‚ñ∂Ô∏è Executing aggregation‚Ä¶")
            cursor = self.col.aggregate(pipeline, maxTimeMS=6_000)
            root   = (await cursor.to_list(length=1))[0] if cursor else {}

            # üëÄ Optional deep‚Äëdebug
            # logger.debug("[SearchRepo] Raw docs: %s", root.get("docs", [])[:2])

            docs  = [
                filter_inventory_summary(doc, store_object_id)
                for doc in root.get("docs", [])
            ]
            total = int(root.get("total", 0))

            logger.info("[SearchRepo] ‚úÖ Returned %d docs | total=%d", len(docs), total)
            return docs, total

        except Exception as exc:  # pragma: no cover
            logger.error("[SearchRepo] üí• Aggregation failed: %s", exc)
            raise InfrastructureError(str(exc)) from exc
